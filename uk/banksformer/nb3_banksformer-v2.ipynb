{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook trains Banksformer models and generates sythetic data.   \n",
    "Parameters for generating data (seq_len, number of seqs) are near bottom (Under \"Generate Full dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set input dataset and nb_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_KEY_ORDER is ['tcode_num', 'dow', 'month', 'day', 'dtme', 'td_sc', 'log_amount_sc']\n",
      "LOSS_TYPES are: day - scce, dtme - scce, dow - scce, month - scce, td_sc - pdf, log_amount_sc - pdf, tcode_num - scce\n",
      "If this is not correct, edit field_config.py and re-run notebook\n"
     ]
    }
   ],
   "source": [
    "from field_config import CLOCK_DIMS, get_field_info, DATA_KEY_ORDER, LOSS_TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_suffix = \"-uk\"\n",
    "nb_id = \"cond\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6983, 21, 54), (6983, 20, 7), (6983,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_tensor = np.load(f\"stored_data/inp_tensor-{ds_suffix}.npy\")\n",
    "tar_tensor = np.load(f\"stored_data/tar_tensor-{ds_suffix}.npy\")\n",
    "attributes = np.load(f\"stored_data/attributes-{ds_suffix}.npy\")\n",
    "\n",
    "inp_tensor.shape, tar_tensor.shape, attributes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seqs, n_steps, n_feat_inp = inp_tensor.shape\n",
    "n_feat_tar = tar_tensor.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.encoding import load_data_encoder\n",
    "data_encoder = load_data_encoder(ds_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and create tf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_tr, x_cv, inds_tr, inds_cv, targ_tr, targ_cv = train_test_split(\n",
    "    inp_tensor, np.arange(n_seqs), tar_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-29 11:56:27.712714: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(21, 54), dtype=tf.float32, name=None), TensorSpec(shape=(20, 7), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tr = tf.data.Dataset.from_tensor_slices((x_tr.astype(np.float32), targ_tr.astype(np.float32)))\n",
    "ds_cv = tf.data.Dataset.from_tensor_slices((x_cv.astype(np.float32), targ_cv.astype(np.float32)))\n",
    "\n",
    "ds_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.transformer_core import make_batches\n",
    "\n",
    "BUFFER_SIZE = ds_tr.cardinality().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return  -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf_gen(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.cast(tf.math.log(2. * np.pi), tf.float64)\n",
    "    return  -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, MeanSquaredError, SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "loss_scce_logit = SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "loss_scce_probit = SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction='none')\n",
    "\n",
    "loss_mse = MeanSquaredError(reduction='none')\n",
    "\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(tf.reduce_sum(seq, axis=2), 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_parts = []\n",
    "    loss_parts_weighted = []\n",
    "\n",
    "    for k, k_pred in pred.items():\n",
    "\n",
    "        st = FIELD_STARTS_TAR[k]\n",
    "        end = st + FIELD_DIMS_TAR[k]\n",
    "        loss_type = LOSS_TYPES[k]\n",
    "        \n",
    "\n",
    "        if loss_type == \"scce\":\n",
    "            loss_ = loss_scce_logit(real[:, :, st:end], k_pred)\n",
    "        elif loss_type == \"clock\":\n",
    "            loss_ = loss_scce_probit(real[:, :, st:end], clock_to_onehot(k, k_pred))\n",
    "        elif loss_type == \"mse\":\n",
    "            loss_ = loss_mse(real[:, :, st:end], k_pred)\n",
    "        elif loss_type == \"pdf\":\n",
    "            loss_ = -log_normal_pdf(real[:, :, st:end], k_pred[:,:,0:1], k_pred[:,:,1:2])[:,:,0]\n",
    "        else:\n",
    "            raise Exception(f\"Invalid loss type! Got loss type = {loss_type} with key = {k}. Check field_config.py for loss types\")\n",
    "            \n",
    "\n",
    "        mask = tf.math.logical_not(tf.math.equal(tf.reduce_sum(real, axis=2), 0))\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        loss_ = tf.reduce_sum(loss_)/tf.reduce_sum(mask) \n",
    "\n",
    "        loss_parts.append(loss_)\n",
    "        loss_parts_weighted.append(loss_ * LOSS_WEIGHTS[k])\n",
    "\n",
    "    return tf.reduce_sum(loss_parts_weighted), loss_parts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day': 31, 'dtme': 31, 'dow': 7, 'month': 12}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from my_lib.encoding import bulk_encode_time_value\n",
    "\n",
    "EPS_CLOCKP = 0.01\n",
    "\n",
    "CLOCKS = {}\n",
    "for k, val in CLOCK_DIMS.items():\n",
    "    CLOCKS[k] = tf.constant(bulk_encode_time_value(np.arange(val), val), dtype=tf.float32)\n",
    "\n",
    "def clock_to_probs(pt, pts):\n",
    "    \n",
    "    ds = tf.constant(pts) - pt\n",
    "    sq_ds = np.sum(tf.square(ds+EPS_CLOCKP), axis=1)\n",
    "    raw_ps = 1/ sq_ds   \n",
    "    \n",
    "    return raw_ps / np.sum(raw_ps)\n",
    "\n",
    "\n",
    "\n",
    "def clock_to_onehot(k, vals):\n",
    "    orig_shape = vals.shape\n",
    "\n",
    "    vals = tf.reshape(vals, (-1, orig_shape[-1]))\n",
    "\n",
    "    return np.array([clock_to_probs(p, CLOCKS[k]) for p in vals]).reshape(*orig_shape[:-1], -1)   \n",
    "\n",
    "\n",
    "CLOCK_DIMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Banksformer configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATIONS = {\n",
    "    \"td_sc\": \"relu\",\n",
    "    \"log_amount_sc\": \"relu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "FIELD_DIMS_IN, FIELD_STARTS_IN, FIELD_DIMS_TAR, FIELD_STARTS_TAR, FIELD_DIMS_NET, FIELD_STARTS_NET = get_field_info(ds_suffix)\n",
    "\n",
    "config[\"ORDER\"] = DATA_KEY_ORDER\n",
    "config[\"FIELD_STARTS_IN\"] = FIELD_STARTS_IN\n",
    "config[\"FIELD_DIMS_IN\"] = FIELD_DIMS_IN\n",
    "config[\"FIELD_STARTS_NET\"] = FIELD_STARTS_NET\n",
    "config[\"FIELD_DIMS_NET\"] = FIELD_DIMS_NET\n",
    "\n",
    "\n",
    "config[\"ACTIVATIONS\"] = ACTIVATIONS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:56\n",
      "Begin running v2b__nld_4-dm_128-nh_2-i_0-dr_0.1-opt_adam-lwi_0-bs_64\n",
      "Epoch 1 Batch 0 Loss 12.0507\n",
      "Epoch 1 Batch 50 Loss 10.5404\n",
      "Epoch 1 Loss 10.1559\n",
      "** on validation data loss is 9.3292\n",
      "Not recording acc: 'Transformer' object has no attribute 'acc_function'\n",
      "Time taken for 1 epoch: 98.31 secs\n",
      "\n",
      "Saving checkpoint for epoch 1 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-1\n",
      "Epoch 2 Batch 0 Loss 9.3089\n",
      "Epoch 2 Batch 50 Loss 9.2354\n",
      "Epoch 2 Loss 8.9652\n",
      "** on validation data loss is 7.0959\n",
      "Time taken for 1 epoch: 100.06 secs\n",
      "\n",
      "Saving checkpoint for epoch 2 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-2\n",
      "Epoch 3 Batch 0 Loss 7.0371\n",
      "Epoch 3 Batch 50 Loss 7.2459\n",
      "Epoch 3 Loss 7.1521\n",
      "** on validation data loss is 6.8766\n",
      "Time taken for 1 epoch: 114.98 secs\n",
      "\n",
      "Saving checkpoint for epoch 3 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-3\n",
      "Epoch 4 Batch 0 Loss 6.7960\n",
      "Epoch 4 Batch 50 Loss 6.8877\n",
      "Epoch 4 Loss 6.8305\n",
      "** on validation data loss is 6.7172\n",
      "Time taken for 1 epoch: 97.66 secs\n",
      "\n",
      "Saving checkpoint for epoch 4 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-4\n",
      "Epoch 5 Batch 0 Loss 6.7142\n",
      "Epoch 5 Batch 50 Loss 6.6954\n",
      "Epoch 5 Loss 6.6485\n",
      "** on validation data loss is 6.6321\n",
      "Time taken for 1 epoch: 110.00 secs\n",
      "\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-5\n",
      "Epoch 6 Batch 0 Loss 6.7439\n",
      "Epoch 6 Batch 50 Loss 6.5610\n",
      "Epoch 6 Loss 6.5227\n",
      "** on validation data loss is 6.4657\n",
      "Time taken for 1 epoch: 108.71 secs\n",
      "\n",
      "Saving checkpoint for epoch 6 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-6\n",
      "Epoch 7 Batch 0 Loss 6.5407\n",
      "Epoch 7 Batch 50 Loss 6.4211\n",
      "Epoch 7 Loss 6.4058\n",
      "** on validation data loss is 6.3817\n",
      "Time taken for 1 epoch: 103.99 secs\n",
      "\n",
      "Saving checkpoint for epoch 7 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-7\n",
      "Epoch 8 Batch 0 Loss 6.3740\n",
      "Epoch 8 Batch 50 Loss 6.3247\n",
      "Epoch 8 Loss 6.2918\n",
      "** on validation data loss is 6.2647\n",
      "Time taken for 1 epoch: 96.99 secs\n",
      "\n",
      "Saving checkpoint for epoch 8 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-8\n",
      "Epoch 9 Batch 0 Loss 6.2769\n",
      "Epoch 9 Batch 50 Loss 6.2037\n",
      "Epoch 9 Loss 6.1866\n",
      "** on validation data loss is 6.1398\n",
      "Time taken for 1 epoch: 97.26 secs\n",
      "\n",
      "Saving checkpoint for epoch 9 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-9\n",
      "Epoch 10 Batch 0 Loss 6.1952\n",
      "Epoch 10 Batch 50 Loss 6.1198\n",
      "Epoch 10 Loss 6.1002\n",
      "** on validation data loss is 6.0522\n",
      "Time taken for 1 epoch: 103.64 secs\n",
      "\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-10\n",
      "Epoch 11 Batch 0 Loss 6.0979\n",
      "Epoch 11 Batch 50 Loss 6.0393\n",
      "Epoch 11 Loss 6.0250\n",
      "** on validation data loss is 6.0205\n",
      "Time taken for 1 epoch: 61.06 secs\n",
      "\n",
      "Saving checkpoint for epoch 11 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-11\n",
      "Epoch 12 Batch 0 Loss 5.9031\n",
      "Epoch 12 Batch 50 Loss 5.9757\n",
      "Epoch 12 Loss 5.9626\n",
      "** on validation data loss is 5.8799\n",
      "Time taken for 1 epoch: 62.49 secs\n",
      "\n",
      "Saving checkpoint for epoch 12 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-12\n",
      "Epoch 13 Batch 0 Loss 5.8732\n",
      "Epoch 13 Batch 50 Loss 5.9049\n",
      "Epoch 13 Loss 5.9027\n",
      "** on validation data loss is 5.8583\n",
      "Time taken for 1 epoch: 83.19 secs\n",
      "\n",
      "Saving checkpoint for epoch 13 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-13\n",
      "Epoch 14 Batch 0 Loss 5.9342\n",
      "Epoch 14 Batch 50 Loss 5.8643\n",
      "Epoch 14 Loss 5.8492\n",
      "** on validation data loss is 5.8068\n",
      "Time taken for 1 epoch: 55.32 secs\n",
      "\n",
      "Saving checkpoint for epoch 14 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-14\n",
      "Epoch 15 Batch 0 Loss 5.6979\n",
      "Epoch 15 Batch 50 Loss 5.8099\n",
      "Epoch 15 Loss 5.8072\n",
      "** on validation data loss is 5.7537\n",
      "Time taken for 1 epoch: 56.75 secs\n",
      "\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-15\n",
      "Epoch 16 Batch 0 Loss 5.6831\n",
      "Epoch 16 Batch 50 Loss 5.7668\n",
      "Epoch 16 Loss 5.7629\n",
      "** on validation data loss is 5.7306\n",
      "Time taken for 1 epoch: 59.24 secs\n",
      "\n",
      "Saving checkpoint for epoch 16 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-16\n",
      "Epoch 17 Batch 0 Loss 5.7260\n",
      "Epoch 17 Batch 50 Loss 5.7241\n",
      "Epoch 17 Loss 5.7234\n",
      "** on validation data loss is 5.7168\n",
      "Time taken for 1 epoch: 56.08 secs\n",
      "\n",
      "Saving checkpoint for epoch 17 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-17\n",
      "Epoch 18 Batch 0 Loss 5.7722\n",
      "Epoch 18 Batch 50 Loss 5.6890\n",
      "Epoch 18 Loss 5.6860\n",
      "** on validation data loss is 5.7325\n",
      "Time taken for 1 epoch: 56.31 secs\n",
      "\n",
      "Saving checkpoint for epoch 18 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-18\n",
      "Epoch 19 Batch 0 Loss 5.8062\n",
      "Epoch 19 Batch 50 Loss 5.6622\n",
      "Epoch 19 Loss 5.6529\n",
      "** on validation data loss is 5.6295\n",
      "Time taken for 1 epoch: 60.66 secs\n",
      "\n",
      "Saving checkpoint for epoch 19 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-19\n",
      "Epoch 20 Batch 0 Loss 5.6841\n",
      "Epoch 20 Batch 50 Loss 5.6315\n",
      "Epoch 20 Loss 5.6198\n",
      "** on validation data loss is 5.6076\n",
      "Time taken for 1 epoch: 55.98 secs\n",
      "\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-20\n",
      "Epoch 21 Batch 0 Loss 5.6958\n",
      "Epoch 21 Batch 50 Loss 5.6137\n",
      "Epoch 21 Loss 5.5936\n",
      "** on validation data loss is 5.5824\n",
      "Time taken for 1 epoch: 50.74 secs\n",
      "\n",
      "Saving checkpoint for epoch 21 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-21\n",
      "Epoch 22 Batch 0 Loss 5.5078\n",
      "Epoch 22 Batch 50 Loss 5.5664\n",
      "Epoch 22 Loss 5.5691\n",
      "** on validation data loss is 5.5972\n",
      "Time taken for 1 epoch: 57.39 secs\n",
      "\n",
      "Saving checkpoint for epoch 22 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-22\n",
      "Epoch 23 Batch 0 Loss 5.6891\n",
      "Epoch 23 Batch 50 Loss 5.5449\n",
      "Epoch 23 Loss 5.5496\n",
      "** on validation data loss is 5.5648\n",
      "Time taken for 1 epoch: 55.75 secs\n",
      "\n",
      "Saving checkpoint for epoch 23 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-23\n",
      "Epoch 24 Batch 0 Loss 5.5421\n",
      "Epoch 24 Batch 50 Loss 5.5332\n",
      "Epoch 24 Loss 5.5296\n",
      "** on validation data loss is 5.5349\n",
      "Time taken for 1 epoch: 57.59 secs\n",
      "\n",
      "Saving checkpoint for epoch 24 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-24\n",
      "Epoch 25 Batch 0 Loss 5.4992\n",
      "Epoch 25 Batch 50 Loss 5.5010\n",
      "Epoch 25 Loss 5.5011\n",
      "** on validation data loss is 5.5179\n",
      "Time taken for 1 epoch: 52.13 secs\n",
      "\n",
      "Saving checkpoint for epoch 25 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-25\n",
      "Epoch 26 Batch 0 Loss 5.6191\n",
      "Epoch 26 Batch 50 Loss 5.4910\n",
      "Epoch 26 Loss 5.4797\n",
      "** on validation data loss is 5.4557\n",
      "Time taken for 1 epoch: 49.69 secs\n",
      "\n",
      "Saving checkpoint for epoch 26 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-26\n",
      "Epoch 27 Batch 0 Loss 5.4872\n",
      "Epoch 27 Batch 50 Loss 5.4610\n",
      "Epoch 27 Loss 5.4540\n",
      "** on validation data loss is 5.5098\n",
      "Time taken for 1 epoch: 51.11 secs\n",
      "\n",
      "Saving checkpoint for epoch 27 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-27\n",
      "Epoch 28 Batch 0 Loss 5.5023\n",
      "Epoch 28 Batch 50 Loss 5.4232\n",
      "Epoch 28 Loss 5.4287\n",
      "** on validation data loss is 5.4270\n",
      "Time taken for 1 epoch: 48.05 secs\n",
      "\n",
      "Saving checkpoint for epoch 28 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-28\n",
      "Epoch 29 Batch 0 Loss 5.4869\n",
      "Epoch 29 Batch 50 Loss 5.3991\n",
      "Epoch 29 Loss 5.4044\n",
      "** on validation data loss is 5.4231\n",
      "Time taken for 1 epoch: 48.67 secs\n",
      "\n",
      "Saving checkpoint for epoch 29 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-29\n",
      "Epoch 30 Batch 0 Loss 5.4371\n",
      "Epoch 30 Batch 50 Loss 5.3832\n",
      "Epoch 30 Loss 5.3819\n",
      "** on validation data loss is 5.3816\n",
      "Time taken for 1 epoch: 49.81 secs\n",
      "\n",
      "Saving checkpoint for epoch 30 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-30\n",
      "Epoch 31 Batch 0 Loss 5.3590\n",
      "Epoch 31 Batch 50 Loss 5.3575\n",
      "Epoch 31 Loss 5.3606\n",
      "** on validation data loss is 5.3799\n",
      "Time taken for 1 epoch: 50.69 secs\n",
      "\n",
      "Saving checkpoint for epoch 31 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-31\n",
      "Epoch 32 Batch 0 Loss 5.3886\n",
      "Epoch 32 Batch 50 Loss 5.3484\n",
      "Epoch 32 Loss 5.3396\n",
      "** on validation data loss is 5.3704\n",
      "Time taken for 1 epoch: 30.53 secs\n",
      "\n",
      "Saving checkpoint for epoch 32 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-32\n",
      "Epoch 33 Batch 0 Loss 5.3537\n",
      "Epoch 33 Batch 50 Loss 5.3189\n",
      "Epoch 33 Loss 5.3196\n",
      "** on validation data loss is 5.3987\n",
      "Time taken for 1 epoch: 27.36 secs\n",
      "\n",
      "Saving checkpoint for epoch 33 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-33\n",
      "Epoch 34 Batch 0 Loss 5.4137\n",
      "Epoch 34 Batch 50 Loss 5.3064\n",
      "Epoch 34 Loss 5.3091\n",
      "** on validation data loss is 5.3462\n",
      "Time taken for 1 epoch: 28.19 secs\n",
      "\n",
      "Saving checkpoint for epoch 34 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-34\n",
      "Epoch 35 Batch 0 Loss 5.2605\n",
      "Epoch 35 Batch 50 Loss 5.2827\n",
      "Epoch 35 Loss 5.2868\n",
      "** on validation data loss is 5.3286\n",
      "Time taken for 1 epoch: 27.99 secs\n",
      "\n",
      "Saving checkpoint for epoch 35 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-35\n",
      "Epoch 36 Batch 0 Loss 5.3119\n",
      "Epoch 36 Batch 50 Loss 5.2595\n",
      "Epoch 36 Loss 5.2741\n",
      "** on validation data loss is 5.2981\n",
      "Time taken for 1 epoch: 27.43 secs\n",
      "\n",
      "Saving checkpoint for epoch 36 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-36\n",
      "Epoch 37 Batch 0 Loss 5.3271\n",
      "Epoch 37 Batch 50 Loss 5.2664\n",
      "Epoch 37 Loss 5.2579\n",
      "** on validation data loss is 5.2919\n",
      "Time taken for 1 epoch: 27.50 secs\n",
      "\n",
      "Saving checkpoint for epoch 37 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-37\n",
      "Epoch 38 Batch 0 Loss 5.3666\n",
      "Epoch 38 Batch 50 Loss 5.2377\n",
      "Epoch 38 Loss 5.2494\n",
      "** on validation data loss is 5.3564\n",
      "Time taken for 1 epoch: 27.20 secs\n",
      "\n",
      "Saving checkpoint for epoch 38 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-38\n",
      "Epoch 39 Batch 0 Loss 5.3322\n",
      "Epoch 39 Batch 50 Loss 5.2255\n",
      "Epoch 39 Loss 5.2339\n",
      "** on validation data loss is 5.2961\n",
      "Time taken for 1 epoch: 27.01 secs\n",
      "\n",
      "Stopping early, last 2 val losses are: [<tf.Tensor: shape=(), dtype=float32, numpy=5.3563995>, <tf.Tensor: shape=(), dtype=float32, numpy=5.296116>]                       \n",
      "Best was 5.292\n",
      "\n",
      "\n",
      "Wrote transformer.results to training_history/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64.pickle\n",
      "12:37\n",
      "Begin running v2b__nld_4-dm_128-nh_2-i_1-dr_0.1-opt_adam-lwi_0-bs_64\n",
      "Epoch 1 Batch 0 Loss 13.0305\n",
      "Epoch 1 Batch 50 Loss 10.2116\n",
      "Epoch 1 Loss 9.9750\n",
      "** on validation data loss is 9.4497\n",
      "Not recording acc: 'Transformer' object has no attribute 'acc_function'\n",
      "Time taken for 1 epoch: 30.93 secs\n",
      "\n",
      "Saving checkpoint for epoch 1 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-1\n",
      "Epoch 2 Batch 0 Loss 9.3848\n",
      "Epoch 2 Batch 50 Loss 9.1981\n",
      "Epoch 2 Loss 9.0871\n",
      "** on validation data loss is 9.0726\n",
      "Time taken for 1 epoch: 28.32 secs\n",
      "\n",
      "Saving checkpoint for epoch 2 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-2\n",
      "Epoch 3 Batch 0 Loss 9.0002\n",
      "Epoch 3 Batch 50 Loss 8.2248\n",
      "Epoch 3 Loss 7.7189\n",
      "** on validation data loss is 6.9679\n",
      "Time taken for 1 epoch: 28.62 secs\n",
      "\n",
      "Saving checkpoint for epoch 3 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-3\n",
      "Epoch 4 Batch 0 Loss 6.9679\n",
      "Epoch 4 Batch 50 Loss 6.8370\n",
      "Epoch 4 Loss 6.7789\n",
      "** on validation data loss is 6.9446\n",
      "Time taken for 1 epoch: 28.81 secs\n",
      "\n",
      "Saving checkpoint for epoch 4 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-4\n",
      "Epoch 5 Batch 0 Loss 7.0129\n",
      "Epoch 5 Batch 50 Loss 6.6327\n",
      "Epoch 5 Loss 6.5800\n",
      "** on validation data loss is 6.4174\n",
      "Time taken for 1 epoch: 28.99 secs\n",
      "\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-5\n",
      "Epoch 6 Batch 0 Loss 6.3261\n",
      "Epoch 6 Batch 50 Loss 6.4270\n",
      "Epoch 6 Loss 6.3974\n",
      "** on validation data loss is 6.2923\n",
      "Time taken for 1 epoch: 28.87 secs\n",
      "\n",
      "Saving checkpoint for epoch 6 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-6\n",
      "Epoch 7 Batch 0 Loss 6.2275\n",
      "Epoch 7 Batch 50 Loss 6.2890\n",
      "Epoch 7 Loss 6.2549\n",
      "** on validation data loss is 6.2299\n",
      "Time taken for 1 epoch: 29.57 secs\n",
      "\n",
      "Saving checkpoint for epoch 7 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-7\n",
      "Epoch 8 Batch 0 Loss 6.3549\n",
      "Epoch 8 Batch 50 Loss 6.1757\n",
      "Epoch 8 Loss 6.1605\n",
      "** on validation data loss is 6.1117\n",
      "Time taken for 1 epoch: 28.97 secs\n",
      "\n",
      "Saving checkpoint for epoch 8 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-8\n",
      "Epoch 9 Batch 0 Loss 6.0846\n",
      "Epoch 9 Batch 50 Loss 6.0681\n",
      "Epoch 9 Loss 6.0599\n",
      "** on validation data loss is 5.9922\n",
      "Time taken for 1 epoch: 29.12 secs\n",
      "\n",
      "Saving checkpoint for epoch 9 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-9\n",
      "Epoch 10 Batch 0 Loss 5.8409\n",
      "Epoch 10 Batch 50 Loss 6.0004\n",
      "Epoch 10 Loss 5.9821\n",
      "** on validation data loss is 5.8978\n",
      "Time taken for 1 epoch: 31.45 secs\n",
      "\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-10\n",
      "Epoch 11 Batch 0 Loss 5.8883\n",
      "Epoch 11 Batch 50 Loss 5.9306\n",
      "Epoch 11 Loss 5.9099\n",
      "** on validation data loss is 5.9113\n",
      "Time taken for 1 epoch: 35.68 secs\n",
      "\n",
      "Saving checkpoint for epoch 11 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-11\n",
      "Epoch 12 Batch 0 Loss 5.9187\n",
      "Epoch 12 Batch 50 Loss 5.8620\n",
      "Epoch 12 Loss 5.8526\n",
      "** on validation data loss is 5.8498\n",
      "Time taken for 1 epoch: 39.87 secs\n",
      "\n",
      "Saving checkpoint for epoch 12 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-12\n",
      "Epoch 13 Batch 0 Loss 5.8263\n",
      "Epoch 13 Batch 50 Loss 5.7971\n",
      "Epoch 13 Loss 5.7860\n",
      "** on validation data loss is 5.7490\n",
      "Time taken for 1 epoch: 34.84 secs\n",
      "\n",
      "Saving checkpoint for epoch 13 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-13\n",
      "Epoch 14 Batch 0 Loss 5.8730\n",
      "Epoch 14 Batch 50 Loss 5.7298\n",
      "Epoch 14 Loss 5.7373\n",
      "** on validation data loss is 5.6737\n",
      "Time taken for 1 epoch: 33.27 secs\n",
      "\n",
      "Saving checkpoint for epoch 14 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-14\n",
      "Epoch 15 Batch 0 Loss 5.5382\n",
      "Epoch 15 Batch 50 Loss 5.6992\n",
      "Epoch 15 Loss 5.6830\n",
      "** on validation data loss is 5.6607\n",
      "Time taken for 1 epoch: 32.13 secs\n",
      "\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-15\n",
      "Epoch 16 Batch 0 Loss 5.6335\n",
      "Epoch 16 Batch 50 Loss 5.6528\n",
      "Epoch 16 Loss 5.6399\n",
      "** on validation data loss is 5.6316\n",
      "Time taken for 1 epoch: 32.63 secs\n",
      "\n",
      "Saving checkpoint for epoch 16 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-16\n",
      "Epoch 17 Batch 0 Loss 5.6387\n",
      "Epoch 17 Batch 50 Loss 5.5937\n",
      "Epoch 17 Loss 5.5894\n",
      "** on validation data loss is 5.5354\n",
      "Time taken for 1 epoch: 32.54 secs\n",
      "\n",
      "Saving checkpoint for epoch 17 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-17\n",
      "Epoch 18 Batch 0 Loss 5.5447\n",
      "Epoch 18 Batch 50 Loss 5.5437\n",
      "Epoch 18 Loss 5.5379\n",
      "** on validation data loss is 5.5483\n",
      "Time taken for 1 epoch: 31.43 secs\n",
      "\n",
      "Saving checkpoint for epoch 18 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-18\n",
      "Epoch 19 Batch 0 Loss 5.5689\n",
      "Epoch 19 Batch 50 Loss 5.5006\n",
      "Epoch 19 Loss 5.4948\n",
      "** on validation data loss is 5.4980\n",
      "Time taken for 1 epoch: 32.11 secs\n",
      "\n",
      "Saving checkpoint for epoch 19 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-19\n",
      "Epoch 20 Batch 0 Loss 5.5238\n",
      "Epoch 20 Batch 50 Loss 5.4567\n",
      "Epoch 20 Loss 5.4524\n",
      "** on validation data loss is 5.4243\n",
      "Time taken for 1 epoch: 31.81 secs\n",
      "\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-20\n",
      "Epoch 21 Batch 0 Loss 5.3971\n",
      "Epoch 21 Batch 50 Loss 5.4094\n",
      "Epoch 21 Loss 5.4126\n",
      "** on validation data loss is 5.4608\n",
      "Time taken for 1 epoch: 31.05 secs\n",
      "\n",
      "Saving checkpoint for epoch 21 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-21\n",
      "Epoch 22 Batch 0 Loss 5.4898\n",
      "Epoch 22 Batch 50 Loss 5.3876\n",
      "Epoch 22 Loss 5.3832\n",
      "** on validation data loss is 5.4545\n",
      "Time taken for 1 epoch: 30.80 secs\n",
      "\n",
      "Stopping early, last 2 val losses are: [<tf.Tensor: shape=(), dtype=float32, numpy=5.4608192>, <tf.Tensor: shape=(), dtype=float32, numpy=5.454503>]                       \n",
      "Best was 5.424\n",
      "\n",
      "\n",
      "Wrote transformer.results to training_history/v2b__nld_4-dm_128-nh_2-i_1-dr_0__1-opt_adam-lwi_0-bs_64.pickle\n",
      "12:48\n",
      "Begin running v2b__nld_4-dm_128-nh_2-i_2-dr_0.1-opt_adam-lwi_0-bs_64\n",
      "Epoch 1 Batch 0 Loss 12.2463\n",
      "Epoch 1 Batch 50 Loss 9.3061\n",
      "Epoch 1 Loss 8.6866\n",
      "** on validation data loss is 7.7910\n",
      "Not recording acc: 'Transformer' object has no attribute 'acc_function'\n",
      "Time taken for 1 epoch: 31.16 secs\n",
      "\n",
      "Saving checkpoint for epoch 1 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-1\n",
      "Epoch 2 Batch 0 Loss 7.7308\n",
      "Epoch 2 Batch 50 Loss 7.3310\n",
      "Epoch 2 Loss 7.1933\n",
      "** on validation data loss is 6.8355\n",
      "Time taken for 1 epoch: 32.10 secs\n",
      "\n",
      "Saving checkpoint for epoch 2 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-2\n",
      "Epoch 3 Batch 0 Loss 6.9438\n",
      "Epoch 3 Batch 50 Loss 6.8459\n",
      "Epoch 3 Loss 6.8099\n",
      "** on validation data loss is 6.6656\n",
      "Time taken for 1 epoch: 32.29 secs\n",
      "\n",
      "Saving checkpoint for epoch 3 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-3\n",
      "Epoch 4 Batch 0 Loss 6.7803\n",
      "Epoch 4 Batch 50 Loss 6.6727\n",
      "Epoch 4 Loss 6.6339\n",
      "** on validation data loss is 6.5088\n",
      "Time taken for 1 epoch: 32.40 secs\n",
      "\n",
      "Saving checkpoint for epoch 4 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-4\n",
      "Epoch 5 Batch 0 Loss 6.7014\n",
      "Epoch 5 Batch 50 Loss 6.4985\n",
      "Epoch 5 Loss 6.4700\n",
      "** on validation data loss is 6.5057\n",
      "Time taken for 1 epoch: 31.87 secs\n",
      "\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-5\n",
      "Epoch 6 Batch 0 Loss 6.3827\n",
      "Epoch 6 Batch 50 Loss 6.3420\n",
      "Epoch 6 Loss 6.3126\n",
      "** on validation data loss is 6.2429\n",
      "Time taken for 1 epoch: 32.60 secs\n",
      "\n",
      "Saving checkpoint for epoch 6 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-6\n",
      "Epoch 7 Batch 0 Loss 6.2336\n",
      "Epoch 7 Batch 50 Loss 6.1944\n",
      "Epoch 7 Loss 6.1719\n",
      "** on validation data loss is 6.1567\n",
      "Time taken for 1 epoch: 33.18 secs\n",
      "\n",
      "Saving checkpoint for epoch 7 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-7\n",
      "Epoch 8 Batch 0 Loss 6.1606\n",
      "Epoch 8 Batch 50 Loss 6.0919\n",
      "Epoch 8 Loss 6.0774\n",
      "** on validation data loss is 6.0372\n",
      "Time taken for 1 epoch: 32.54 secs\n",
      "\n",
      "Saving checkpoint for epoch 8 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-8\n",
      "Epoch 9 Batch 0 Loss 6.0141\n",
      "Epoch 9 Batch 50 Loss 6.0125\n",
      "Epoch 9 Loss 6.0030\n",
      "** on validation data loss is 6.0240\n",
      "Time taken for 1 epoch: 33.06 secs\n",
      "\n",
      "Saving checkpoint for epoch 9 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-9\n",
      "Epoch 10 Batch 0 Loss 5.9655\n",
      "Epoch 10 Batch 50 Loss 5.9424\n",
      "Epoch 10 Loss 5.9316\n",
      "** on validation data loss is 5.9439\n",
      "Time taken for 1 epoch: 34.09 secs\n",
      "\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-10\n",
      "Epoch 11 Batch 0 Loss 5.9862\n",
      "Epoch 11 Batch 50 Loss 5.8836\n",
      "Epoch 11 Loss 5.8748\n",
      "** on validation data loss is 5.8259\n",
      "Time taken for 1 epoch: 34.24 secs\n",
      "\n",
      "Saving checkpoint for epoch 11 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-11\n",
      "Epoch 12 Batch 0 Loss 5.8329\n",
      "Epoch 12 Batch 50 Loss 5.8302\n",
      "Epoch 12 Loss 5.8245\n",
      "** on validation data loss is 5.9670\n",
      "Time taken for 1 epoch: 33.46 secs\n",
      "\n",
      "Saving checkpoint for epoch 12 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-12\n",
      "Epoch 13 Batch 0 Loss 6.0480\n",
      "Epoch 13 Batch 50 Loss 5.7880\n",
      "Epoch 13 Loss 5.7785\n",
      "** on validation data loss is 5.7768\n",
      "Time taken for 1 epoch: 32.91 secs\n",
      "\n",
      "Saving checkpoint for epoch 13 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-13\n",
      "Epoch 14 Batch 0 Loss 5.8164\n",
      "Epoch 14 Batch 50 Loss 5.7361\n",
      "Epoch 14 Loss 5.7328\n",
      "** on validation data loss is 5.7576\n",
      "Time taken for 1 epoch: 32.69 secs\n",
      "\n",
      "Saving checkpoint for epoch 14 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-14\n",
      "Epoch 15 Batch 0 Loss 5.8507\n",
      "Epoch 15 Batch 50 Loss 5.7000\n",
      "Epoch 15 Loss 5.6878\n",
      "** on validation data loss is 5.6459\n",
      "Time taken for 1 epoch: 32.83 secs\n",
      "\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-15\n",
      "Epoch 16 Batch 0 Loss 5.7038\n",
      "Epoch 16 Batch 50 Loss 5.6522\n",
      "Epoch 16 Loss 5.6453\n",
      "** on validation data loss is 5.6441\n",
      "Time taken for 1 epoch: 31.82 secs\n",
      "\n",
      "Saving checkpoint for epoch 16 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-16\n",
      "Epoch 17 Batch 0 Loss 5.6434\n",
      "Epoch 17 Batch 50 Loss 5.6210\n",
      "Epoch 17 Loss 5.6109\n",
      "** on validation data loss is 5.6352\n",
      "Time taken for 1 epoch: 32.15 secs\n",
      "\n",
      "Saving checkpoint for epoch 17 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-17\n",
      "Epoch 18 Batch 0 Loss 5.6365\n",
      "Epoch 18 Batch 50 Loss 5.5818\n",
      "Epoch 18 Loss 5.5688\n",
      "** on validation data loss is 5.5688\n",
      "Time taken for 1 epoch: 32.71 secs\n",
      "\n",
      "Saving checkpoint for epoch 18 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-18\n",
      "Epoch 19 Batch 0 Loss 5.6313\n",
      "Epoch 19 Batch 50 Loss 5.5294\n",
      "Epoch 19 Loss 5.5269\n",
      "** on validation data loss is 5.5000\n",
      "Time taken for 1 epoch: 32.89 secs\n",
      "\n",
      "Saving checkpoint for epoch 19 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-19\n",
      "Epoch 20 Batch 0 Loss 5.6545\n",
      "Epoch 20 Batch 50 Loss 5.4874\n",
      "Epoch 20 Loss 5.4802\n",
      "** on validation data loss is 5.4583\n",
      "Time taken for 1 epoch: 32.91 secs\n",
      "\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-20\n",
      "Epoch 21 Batch 0 Loss 5.5437\n",
      "Epoch 21 Batch 50 Loss 5.4529\n",
      "Epoch 21 Loss 5.4478\n",
      "** on validation data loss is 5.4349\n",
      "Time taken for 1 epoch: 31.65 secs\n",
      "\n",
      "Saving checkpoint for epoch 21 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-21\n",
      "Epoch 22 Batch 0 Loss 5.4816\n",
      "Epoch 22 Batch 50 Loss 5.4298\n",
      "Epoch 22 Loss 5.4145\n",
      "** on validation data loss is 5.3825\n",
      "Time taken for 1 epoch: 31.48 secs\n",
      "\n",
      "Saving checkpoint for epoch 22 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-22\n",
      "Epoch 23 Batch 0 Loss 5.4413\n",
      "Epoch 23 Batch 50 Loss 5.3953\n",
      "Epoch 23 Loss 5.3853\n",
      "** on validation data loss is 5.4528\n",
      "Time taken for 1 epoch: 31.26 secs\n",
      "\n",
      "Saving checkpoint for epoch 23 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-23\n",
      "Epoch 24 Batch 0 Loss 5.4962\n",
      "Epoch 24 Batch 50 Loss 5.3591\n",
      "Epoch 24 Loss 5.3613\n",
      "** on validation data loss is 5.3815\n",
      "Time taken for 1 epoch: 31.96 secs\n",
      "\n",
      "Saving checkpoint for epoch 24 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-24\n",
      "Epoch 25 Batch 0 Loss 5.4148\n",
      "Epoch 25 Batch 50 Loss 5.3307\n",
      "Epoch 25 Loss 5.3369\n",
      "** on validation data loss is 5.3224\n",
      "Time taken for 1 epoch: 34.28 secs\n",
      "\n",
      "Saving checkpoint for epoch 25 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-25\n",
      "Epoch 26 Batch 0 Loss 5.2802\n",
      "Epoch 26 Batch 50 Loss 5.3185\n",
      "Epoch 26 Loss 5.3176\n",
      "** on validation data loss is 5.3167\n",
      "Time taken for 1 epoch: 34.93 secs\n",
      "\n",
      "Saving checkpoint for epoch 26 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-26\n",
      "Epoch 27 Batch 0 Loss 5.4438\n",
      "Epoch 27 Batch 50 Loss 5.2991\n",
      "Epoch 27 Loss 5.2987\n",
      "** on validation data loss is 5.2837\n",
      "Time taken for 1 epoch: 28.03 secs\n",
      "\n",
      "Saving checkpoint for epoch 27 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-27\n",
      "Epoch 28 Batch 0 Loss 5.1888\n",
      "Epoch 28 Batch 50 Loss 5.2745\n",
      "Epoch 28 Loss 5.2827\n",
      "** on validation data loss is 5.3490\n",
      "Time taken for 1 epoch: 28.40 secs\n",
      "\n",
      "Saving checkpoint for epoch 28 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-28\n",
      "Epoch 29 Batch 0 Loss 5.4786\n",
      "Epoch 29 Batch 50 Loss 5.2651\n",
      "Epoch 29 Loss 5.2664\n",
      "** on validation data loss is 5.2597\n",
      "Time taken for 1 epoch: 29.30 secs\n",
      "\n",
      "Saving checkpoint for epoch 29 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-29\n",
      "Epoch 30 Batch 0 Loss 5.2675\n",
      "Epoch 30 Batch 50 Loss 5.2549\n",
      "Epoch 30 Loss 5.2537\n",
      "** on validation data loss is 5.2734\n",
      "Time taken for 1 epoch: 30.27 secs\n",
      "\n",
      "Saving checkpoint for epoch 30 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-30\n",
      "Epoch 31 Batch 0 Loss 5.3390\n",
      "Epoch 31 Batch 50 Loss 5.2321\n",
      "Epoch 31 Loss 5.2378\n",
      "** on validation data loss is 5.2964\n",
      "Time taken for 1 epoch: 31.32 secs\n",
      "\n",
      "Stopping early, last 2 val losses are: [<tf.Tensor: shape=(), dtype=float32, numpy=5.273408>, <tf.Tensor: shape=(), dtype=float32, numpy=5.296429>]                       \n",
      "Best was 5.260\n",
      "\n",
      "\n",
      "Wrote transformer.results to training_history/v2b__nld_4-dm_128-nh_2-i_2-dr_0__1-opt_adam-lwi_0-bs_64.pickle\n"
     ]
    }
   ],
   "source": [
    "from my_lib.BanksformerGen import Transformer\n",
    "import pickle \n",
    "\n",
    "\n",
    "all_models = []\n",
    "for_df = []\n",
    "\n",
    "\n",
    "def to_num(x):\n",
    "    try: return int(x)\n",
    "    except: return float(x)\n",
    "\n",
    "    \n",
    "def id_str_to_folder(id_str):\n",
    "    return id_str.replace(\".\", \"__\")\n",
    "beta = 1\n",
    "\n",
    "\n",
    "# moredate\n",
    "LOSS_WEIGHTS_OLD = {\n",
    " 'td_sc':1.,\n",
    " 'year': 0.5,\n",
    " 'month': 0.15,\n",
    " 'day': 0.25,\n",
    " 'dow': 0.1,\n",
    " 'tcode_num': 1.,\n",
    " 'log_amount_sc': 2.}\n",
    "\n",
    "\n",
    "LOSS_WEIGHTS_0 = {\n",
    " 'td_sc':1.,\n",
    " 'month': 0.015,\n",
    " 'day': 0.025,\n",
    " 'dow': 0.01,\n",
    " 'tcode_num': 1.,\n",
    " 'log_amount_sc': 2.}\n",
    "\n",
    "\n",
    "\n",
    "LOSS_WEIGHTS_MID = {\n",
    " 'td_sc':1.,\n",
    " 'month': 0.07,\n",
    " 'day': 0.1,\n",
    " 'dow': 0.04,\n",
    " 'tcode_num': 1.,\n",
    " 'log_amount_sc': 2.}\n",
    "\n",
    "\n",
    "\n",
    "lws = [(LOSS_WEIGHTS_0, \"0\"), (LOSS_WEIGHTS_OLD, \"moredate\")] \n",
    "\n",
    "# lws = [(LOSS_WEIGHTS_MID, \"mid\")]\n",
    "\n",
    "td_loss_fns = [(loss_mse, \"loss_mse\")]\n",
    "\n",
    "\n",
    "EPOCHS = 1\n",
    "EARLY_STOP = 2\n",
    "\n",
    "num_layers_enc = None\n",
    "dropout_rate = 0.1\n",
    "dr = dropout_rate\n",
    "opt_name = \"adam\"\n",
    "# td_loss_fn = loss_mse\n",
    "\n",
    "\n",
    "## Tuning these ! \n",
    "d_model = 128\n",
    "num_layers_dec = 4\n",
    "num_heads = 2\n",
    "bs = 64\n",
    "# lws # above\n",
    "\n",
    "\n",
    "LOSS_WEIGHTS, lwi = lws[0]\n",
    "\n",
    "\n",
    "dff = d_model\n",
    "\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    print(datetime.datetime.now().strftime(\"%H:%M\"))\n",
    "\n",
    "\n",
    "    transformer = Transformer(\n",
    "        num_layers_enc=num_layers_enc, num_layers_dec=num_layers_dec,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dff=dff,\n",
    "        maximum_position_encoding=256,\n",
    "       net_info = None, \n",
    "        inp_dim = n_feat_inp,\n",
    "        final_dim= None,\n",
    "        config=config,\n",
    "        rate=dr)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    transformer.optimizer =  optimizer\n",
    "\n",
    "\n",
    "    train_batches = make_batches(ds_tr, BUFFER_SIZE, bs)\n",
    "\n",
    "\n",
    "    transformer.loss_function = loss_function\n",
    "    LOSS_WEIGHTS[\"dtme\"] = LOSS_WEIGHTS[\"day\"]\n",
    "\n",
    "    LOSS_WEIGHTS[\"k_symbol_num\"] = LOSS_WEIGHTS[\"tcode_num\"]\n",
    "    LOSS_WEIGHTS[\"operation_num\"] = LOSS_WEIGHTS[\"tcode_num\"]\n",
    "    LOSS_WEIGHTS[\"type_num\"] = LOSS_WEIGHTS[\"tcode_num\"]\n",
    "    transformer.LOSS_WEIGHTS = LOSS_WEIGHTS\n",
    "\n",
    "    id_str = f\"v2b__nld_{num_layers_dec}-dm_{d_model}-nh_{num_heads}-i_{i}-dr_{dr}-opt_{opt_name}-lwi_{lwi}-bs_{bs}\"\n",
    "\n",
    "    print(\"Begin running\", id_str)\n",
    "    transformer.id_str = id_str\n",
    "\n",
    "\n",
    "    all_models.append(transformer)\n",
    "    transformer.compile()\n",
    "\n",
    "\n",
    "    transformer.checkpoint_path = f\"./checkpoints/{id_str_to_folder(transformer.id_str)}-{ds_suffix}-{nb_id}\"\n",
    "    # ensure checkpoint directory exists (helpful on Colab)\n",
    "    try:\n",
    "        os.makedirs(transformer.checkpoint_path, exist_ok=True)\n",
    "    except Exception:\n",
    "        # if path creation fails for any reason, continue â€” TF will still try to save\n",
    "        pass\n",
    "\n",
    "    transformer.ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                                       optimizer=optimizer)\n",
    "    transformer.ckpt_manager = tf.train.CheckpointManager(transformer.ckpt, \n",
    "                                                              transformer.checkpoint_path, max_to_keep=EARLY_STOP)\n",
    "\n",
    "    latest_ckpt = transformer.ckpt_manager.latest_checkpoint\n",
    "    if latest_ckpt:\n",
    "        transformer.ckpt.restore(latest_ckpt)\n",
    "        print(f'Latest checkpoint restored from {latest_ckpt}!!')\n",
    "        # try to parse checkpoint index (e.g. '.../ckpt-5') and resume from that index\n",
    "        try:\n",
    "            import re\n",
    "            m = re.search(r'ckpt-(\\d+)$', latest_ckpt)\n",
    "            if m:\n",
    "                start_epoch = int(m.group(1))\n",
    "                print(f\"Resuming from checkpoint index {start_epoch}\")\n",
    "            else:\n",
    "                start_epoch = 0\n",
    "                print(\"Could not parse checkpoint index; starting from 0\")\n",
    "        except Exception:\n",
    "            start_epoch = 0\n",
    "            print(\"Error parsing checkpoint index; starting from 0\")\n",
    "\n",
    "        # resume training from parsed start_epoch\n",
    "        transformer.fit(train_batches, x_cv, targ_cv, epochs=EPOCHS, initial_epoch=start_epoch, early_stop=EARLY_STOP, print_every=50, ckpt_every = 1)\n",
    "    else:\n",
    "        transformer.fit(train_batches, x_cv, targ_cv, epochs= EPOCHS, early_stop=EARLY_STOP, print_every=50, ckpt_every = 1)\n",
    "\n",
    "    transformer.fit_time = time.time() - start\n",
    "    transformer.results[\"fit_time\"] = transformer.fit_time \n",
    "\n",
    "    # ensure training history folder exists before writing\n",
    "    try:\n",
    "        os.makedirs(\"training_history\", exist_ok=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # try to pickle results; sanitize non-serializable entries if needed\n",
    "    safe_results = {}\n",
    "    for k, v in transformer.results.items():\n",
    "        try:\n",
    "            if isinstance(v, (list, tuple)):\n",
    "                new_list = []\n",
    "                for item in v:\n",
    "                    try:\n",
    "                        # tensors / numpy -> primitive\n",
    "                        if hasattr(item, \"numpy\"):\n",
    "                            val = item.numpy()\n",
    "                            try:\n",
    "                                new_list.append(val.tolist())\n",
    "                            except Exception:\n",
    "                                new_list.append(val)\n",
    "                        else:\n",
    "                            new_list.append(item)\n",
    "                    except Exception:\n",
    "                        try:\n",
    "                            new_list.append(float(item))\n",
    "                        except Exception:\n",
    "                            new_list.append(str(item))\n",
    "                safe_results[k] = new_list\n",
    "            else:\n",
    "                if hasattr(v, \"numpy\"):\n",
    "                    safe_results[k] = v.numpy().tolist()\n",
    "                else:\n",
    "                    try:\n",
    "                        safe_results[k] = float(v)\n",
    "                    except Exception:\n",
    "                        safe_results[k] = str(v)\n",
    "        except Exception:\n",
    "            safe_results[k] = str(v)\n",
    "\n",
    "    with open(f\"training_history/{id_str_to_folder(transformer.id_str)}.pickle\", \"wb\") as f:\n",
    "        pickle.dump(safe_results, f) \n",
    "        print(\"Wrote transformer.results to\", f.name)\n",
    "\n",
    "\n",
    "    # guard against empty val_loss when summarizing for for_df\n",
    "    val_losses = transformer.results.get(\"val_loss\", [])\n",
    "    if len(val_losses) > 0:\n",
    "        best_val = np.min(val_losses)\n",
    "    else:\n",
    "        best_val = np.nan\n",
    "\n",
    "    for_df.append((num_layers_dec, d_model, num_heads, i, dr, beta, dff,\n",
    "                   best_val, opt_name, transformer.id_str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(for_df, columns=['num_layers_dec', 'd_model', 'num_heads', 'i', \"dr\", \"beta\", \"dff\",\n",
    "                                                \"val loss\", \"opt name\",\"id_str\"]).sort_values(\"val loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_layers_dec</th>\n",
       "      <th>d_model</th>\n",
       "      <th>num_heads</th>\n",
       "      <th>i</th>\n",
       "      <th>dr</th>\n",
       "      <th>beta</th>\n",
       "      <th>dff</th>\n",
       "      <th>val loss</th>\n",
       "      <th>opt name</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5.259680</td>\n",
       "      <td>adam</td>\n",
       "      <td>v2b__nld_4-dm_128-nh_2-i_2-dr_0.1-opt_adam-lwi_0-bs_64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5.291912</td>\n",
       "      <td>adam</td>\n",
       "      <td>v2b__nld_4-dm_128-nh_2-i_0-dr_0.1-opt_adam-lwi_0-bs_64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5.424337</td>\n",
       "      <td>adam</td>\n",
       "      <td>v2b__nld_4-dm_128-nh_2-i_1-dr_0.1-opt_adam-lwi_0-bs_64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_layers_dec  d_model  num_heads  i   dr  beta  dff  val loss opt name  \\\n",
       "2               4      128          2  2  0.1     1  128  5.259680     adam   \n",
       "0               4      128          2  0  0.1     1  128  5.291912     adam   \n",
       "1               4      128          2  1  0.1     1  128  5.424337     adam   \n",
       "\n",
       "                                                   id_str  \n",
       "2  v2b__nld_4-dm_128-nh_2-i_2-dr_0.1-opt_adam-lwi_0-bs_64  \n",
       "0  v2b__nld_4-dm_128-nh_2-i_0-dr_0.1-opt_adam-lwi_0-bs_64  \n",
       "1  v2b__nld_4-dm_128-nh_2-i_1-dr_0.1-opt_adam-lwi_0-bs_64  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None, \"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(df.sort_values(\"val loss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
